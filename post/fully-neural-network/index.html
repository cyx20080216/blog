<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>全连接神经网络浅析 - cyx20080216的小屋</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="cyx20080216"><meta name=description content="这是一个全连接神经网络的简图，由一个输入层，两个隐藏层和一个输出层构成 在这篇文章中，我将详细的讲讲它和其它类似的神经网络怎么工作，以及反向传"><meta name=generator content="Hugo 0.97.0 with theme even"><link rel=canonical href=https://cyx20080216.github.io/blog/post/fully-neural-network/><link rel=apple-touch-icon sizes=180x180 href=/blog/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=manifest href=/blog/manifest.json><link rel=mask-icon href=/blog/safari-pinned-tab.svg color=#5bbad5><link href=/blog/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="全连接神经网络浅析"><meta property="og:description" content="这是一个全连接神经网络的简图，由一个输入层，两个隐藏层和一个输出层构成 在这篇文章中，我将详细的讲讲它和其它类似的神经网络怎么工作，以及反向传"><meta property="og:type" content="article"><meta property="og:url" content="https://cyx20080216.github.io/blog/post/fully-neural-network/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-04-15T22:41:00+08:00"><meta property="article:modified_time" content="2022-04-16T12:54:00+08:00"><meta itemprop=name content="全连接神经网络浅析"><meta itemprop=description content="这是一个全连接神经网络的简图，由一个输入层，两个隐藏层和一个输出层构成 在这篇文章中，我将详细的讲讲它和其它类似的神经网络怎么工作，以及反向传"><meta itemprop=datePublished content="2022-04-15T22:41:00+08:00"><meta itemprop=dateModified content="2022-04-16T12:54:00+08:00"><meta itemprop=wordCount content="1154"><meta itemprop=keywords content="算法,机器学习,"><meta name=twitter:card content="summary"><meta name=twitter:title content="全连接神经网络浅析"><meta name=twitter:description content="这是一个全连接神经网络的简图，由一个输入层，两个隐藏层和一个输出层构成 在这篇文章中，我将详细的讲讲它和其它类似的神经网络怎么工作，以及反向传"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/blog/ class=logo>cyx20080216的小屋</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/blog/><li class=mobile-menu-item>首页</li></a><a href=/blog/post/><li class=mobile-menu-item>历史</li></a><a href=/blog/tags/><li class=mobile-menu-item>标签</li></a><a href=/blog/categories/><li class=mobile-menu-item>分类</li></a><a href=/blog/abaut/><li class=mobile-menu-item>关于</li></a><a href=/blog/search/><li class=mobile-menu-item>🔍</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/blog/ class=logo>cyx20080216的小屋</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/blog/>首页</a></li><li class=menu-item><a class=menu-item-link href=/blog/post/>历史</a></li><li class=menu-item><a class=menu-item-link href=/blog/tags/>标签</a></li><li class=menu-item><a class=menu-item-link href=/blog/categories/>分类</a></li><li class=menu-item><a class=menu-item-link href=/blog/abaut/>关于</a></li><li class=menu-item><a class=menu-item-link href=/blog/search/>🔍</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>全连接神经网络浅析</h1><div class=post-meta><span class=post-time>2022-04-15</span><div class=post-category><a href=/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><span class=more-meta>约 1154 字</span>
<span class=more-meta>预计阅读 3 分钟</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#神经元>神经元</a></li><li><a href=#约定>约定</a></li><li><a href=#前向传播>前向传播</a></li><li><a href=#训练>训练</a></li><li><a href=#反向传播>反向传播</a></li><li><a href=#总结>总结</a></li><li><a href=#再谈训练>再谈训练</a></li></ul></nav></div></div><div class=post-content><p><img src=https://cyx20080216.github.io/blog/images/neural-network-structure.png alt=全连接神经网络简图></p><p>这是一个全连接神经网络的简图，由一个输入层，两个隐藏层和一个输出层构成</p><p>在这篇文章中，我将详细的讲讲它和其它类似的神经网络怎么工作，以及反向传播的完整推导过程</p><h1 id=神经元>神经元</h1><p>将一个神经元放大来看，大概是这样：</p><p><img src=https://cyx20080216.github.io/blog/images/neural-structure.png alt=神经元简图></p><p>其中， $w_i$ 是权重， $b$ 是偏置， $f(x)$ 是激活函数</p><p>激活函数有很多种，常用的有 Sigmoid, ReLU 等，但它们的目的都是为了增强网络的拟合能力，进行<strong>非线性的拟合</strong></p><p><strong>激活函数必须是非线性的</strong></p><p>令 $n$ 为输入数量， $x_i$ 为每个输入的值</p><p>在激活之前，它的输出为输入加权求和后的值。即 $q=\sum_{i=1}^nw_ix_i+b$</p><p>激活之后，它的输出为： $r=f_k(q)=f(\sum_{i=1}^nw_ix_i+b)$</p><p>请务必记牢这两个式子，后面会频繁地使用它们</p><h1 id=约定>约定</h1><p>网络层数（不包括输入层）： $T$</p><p>网络输入数量： $n_0$</p><p>第 $k$ 层神经元数量： $n_k$</p><p>第 $k$ 层的激活函数： $f_k(x)$</p><p>连接第 $k$ 层第 $i$ 个神经元与上一层第 $j$ 个神经元的权重： $w_{kij}$</p><p>第 $k$ 层第 $i$ 个神经元的偏置： $b_{ki}$</p><p>网络的第 $i$ 个输入： $r_{0i}$</p><p>第 $k$ 层第 $i$ 个神经元激活前的输出： $q_{ki}$</p><p>第 $k$ 层第 $i$ 个神经元激活后的输出： $r_{ki}$</p><h1 id=前向传播>前向传播</h1><p>之前，我们已经知道了每一个神经元的工作方法</p><p>那么这一步十分简单，用一样的方法即可</p><p>$$q_{ki}=\sum_{j=1}^{n_{k-1}}w_{kij}r_{{k-1}j}+b_{ki}$$</p><p>$$r_{ki}=f_k(q_{ki})$$</p><p>最后一层神经元激活后的输出，就是整个神经网络的输出</p><h1 id=训练>训练</h1><p>到目前为止，我们还有一个很大的问题：参数从哪来？</p><p>类似于网络层数、神经元数量这些关于网络结构以及之后要说的训练过程的参数，我们称为<strong>超参数</strong></p><p>这些参数通常由网络设计者来设定</p><p>而权重和偏置，手动设定显然不现实。所以，我们通过让网络自己学习这些参数，这就是<strong>训练</strong></p><p>为了训练神经网络，首先要设计一个损失函数，用来评估模型的准确度</p><p>如 $L(x_1, x_2, &mldr; x_n, y_1, y_2, &mldr; y_n)=\sum_{i=1}^n(x_i-y_i)^2$ 就是一种常用的损失函数</p><p>我们令期望输出的第 $i$ 个值为 $y_i$</p><p>我们令损失值 $l=L(r_{T1}, r_{T2}, &mldr; r_{Tn_T}, y_1, y_2, &mldr; y_{n_T})$</p><p>想要找到最合适的参数，就是找到 $l$ 的低谷</p><p>我们可以通过一些方法求出 $\dfrac{dl}{dw_{kij}}$ 和 $\dfrac{dl}{dw_{bi}}$</p><p>求出了微分，我们就能够知道如何调整参数才能让 $l$ 尽可能快地减小</p><p>这就是<strong>梯度下降法</strong></p><p>我们使用<strong>优化器</strong>这一概念来描述调整的方法，我们令其为 $O(w, d, \eta)$</p><p>最简单的一种就是 $O(w, d, \eta)=w-\eta d$</p><p>其中的 $\eta$ 指学习率，这也是个超参数</p><p>通常情况下，太高的学习率会导致损失值不稳定，太低会导致学习过慢</p><p>一般，我们可以取 $\eta=0.01$</p><p>这样一来，只需不断使</p><p>$$w_{kij}\gets O(w_{kij}, \dfrac{dl}{dw_{kij}}, \eta)$$</p><p>$$b_{ki}\gets O(b_{ki}, \dfrac{dl}{db_{ki}}, \eta)$$</p><p>就可以得到较优的参数</p><h1 id=反向传播>反向传播</h1><p>现在，我们来说说如何求出 $\dfrac{dl}{dw_{kij}}$ 和 $\dfrac{dl}{db_{ki}}$ ，这也是最难的一部分</p><p><strong>高能预警，请有一定的微分基础后再往下读</strong></p><p>首先，根据 $q_{ki}=\sum_{j=1}^{n_{k-1}}w_{kij}r_{{k-1}j}+b_{ki}$ ，我们得出</p><p>$$
\begin{aligned}
\dfrac{dl}{dw_{kij}}&=\dfrac{dl}{dq_{ki}}\cdot\dfrac{dq_{ki}}{dw_{kij}}\
&=\dfrac{dl}{dq_{ki}}\cdot r_{{k-1}j}\
\end{aligned}
$$</p><p>$$
\begin{aligned}
\dfrac{dl}{db_{ki}}&=\dfrac{dl}{dq_{ki}}\cdot\dfrac{dq_{ki}}{db_{ki}}\
&=\dfrac{dl}{dq_{ki}}\
\end{aligned}
$$</p><p>$$
\begin{aligned}
\dfrac{dl}{dq_{ki}}&=\sum_{j=1}^{n_{k+1}}\dfrac{dl}{dq_{{k+1}j}}\cdot\dfrac{dq_{{k+1}j}}{dr_{ki}}\cdot\dfrac{dr_{ki}}{dq_{ki}}\
&=\sum_{j=1}^{n_{k+1}}\dfrac{dl}{dq_{{k+1}j}}\cdot w_{{k+1}ji}\cdot f_k^{&rsquo;}(q_{ki})\
&=(\sum_{j=1}^{n_{k+1}}\dfrac{dl}{dq_{{k+1}j}}\cdot w_{{k+1}ji})\cdot f_k^{&rsquo;}(q_{ki})\
\end{aligned}
$$</p><p>根据 $l=L(r_{T1}, r_{T2}, &mldr; r_{Tn_T}, y_1, y_2, &mldr; y_{n_T})$ ，可以得出</p><p>$$
\begin{aligned}
\dfrac{dl}{dq_{Ti}}&=\dfrac{dl}{dr_{Ti}}\cdot\dfrac{dr_{Ti}}{dq_{Ti}}\
&=\dfrac{dl}{dr_{Ti}}\cdot f_T^{&rsquo;}(q_{Ti})\
\end{aligned}
$$</p><p>得出了这几条公式，我们就能从后往前算出所有的 $\dfrac{dl}{dq_{ki}}$ ，再用它们求出所有的 $\dfrac{dl}{dw_{kij}}$ 和 $\dfrac{dl}{db_{ki}}$ ，用来进行梯度下降</p><h1 id=总结>总结</h1><p>总结起来，重要的公式无外乎这几个：</p><p>$$q_{ki}=\sum_{j=1}^{n_{k-1}}w_{kij}r_{{k-1}j}+b_{ki}$$</p><p>$$r_{ki}=f_k(q_{ki})$$</p><p>$$l=L(r_{T1}, r_{T2}, &mldr; r_{Tn_T}, y_1, y_2, &mldr; y_{n_T})$$</p><p>$$\dfrac{dl}{dq_{Ti}}=\dfrac{dl}{dr_{Ti}}\cdot f_T^{&rsquo;}(q_{Ti})$$</p><p>$$\dfrac{dl}{dq_{ki}}=(\sum_{j=1}^{n_{k+1}}\dfrac{dl}{dq_{{k+1}j}}\cdot w_{{k+1}ji})\cdot f_k^{&rsquo;}(q_{ki})$$</p><p>$$\dfrac{dl}{dw_{kij}}=\dfrac{dl}{dq_{ki}}\cdot r_{{k-1}j}$$</p><p>$$\dfrac{dl}{db_{ki}}=\dfrac{dl}{dq_{ki}}$$</p><p>$$w_{kij}\gets O(w_{kij}, \dfrac{dl}{dw_{kij}}, \eta)$$</p><p>$$b_{ki}\gets O(b_{ki}, \dfrac{dl}{db_{ki}}, \eta)$$</p><h1 id=再谈训练>再谈训练</h1><p>在训练一个模型时，需要将数据集分成两部分：训练集和测试集</p><p>测试集不能以任何形式参与训练，仅用于测试模型准确性</p><p>若模型在训练集上表现优异，但在测试集上表现不佳，可能是出现了<strong>过拟合</strong>，这时可以通过减少网络层数和增加训练集的数据量来解决</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>文章作者</span>
<span class=item-content>cyx20080216</span></p><p class=copyright-item><span class=item-title>上次更新</span>
<span class=item-content>2022-04-16</span></p><p class=copyright-item><span class=item-title>原始文档</span>
<span class=item-content><a class=link-to-markdown href=https://cyx20080216.github.io/blog/post/fully-neural-network/index.md target=_blank>查看本文 Markdown 版本 »</a></span></p><p class=copyright-item><span class=item-title>许可协议</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh target=_blank>署名-非商业性使用-相同方式共享 4.0 国际</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/blog/tags/%E7%AE%97%E6%B3%95/>算法</a>
<a href=/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><nav class=post-nav><a class=next href=/blog/post/linear-regression/><span class="next-text nav-default">线性回归</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><script src=https://utteranc.es/client.js repo=cyx20080216/blog issue-term=pathname theme=github-light crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the <a href=https://github.com/utterance>comments powered by utterances.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:cyx20080216@outlook.com class="iconfont icon-email" title=email></a>
<a href=https://github.com/cyx20080216 class="iconfont icon-github" title=github></a>
<a href=https://cyx20080216.github.io/blog/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2021 -
2022<span class=heart><i class="iconfont icon-heart"></i></span><span>cyx20080216</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/blog/js/main.min.64437849d125a2d603b3e71d6de5225d641a32d17168a58106e0b61852079683.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>